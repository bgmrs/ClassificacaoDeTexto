! pip install unidecode

import pandas as pd
import spacy
import nltk
import re
from nltk.stem import RSLPStemmer
from unidecode import unidecode

!python -m spacy download pt_core_news_sm

nlp = spacy.load('pt_core_news_sm')
nltk.download('stopwords')
nltk.download('rslp')

# Treino

## Análise exploratória dos dados

df = pd.read_csv("../input/5cd-classificacao-emocao/6 - base_dados_treinamento - Kaggle.csv")

df.head()

df.isnull().sum()

df.info()

## Pré - processamento

# Split da coluna "id;Texto;Label" separado por ;
new = df["id;Texto;Label"].str.split(";", n = 2, expand = True)

# Criando a Nova Coluna "Id" com o new[0]
df["id"]= new[0] 

# Criando a Nova Coluna "Texto" com o new[1]
df["Texto"]= new[1]

# Criando a Nova Coluna "Label" com o new[2]
df["Label"]= new[2]

# Retirando a antiga coluna "id;Texto;Label"
df.drop(columns =["id;Texto;Label"], inplace = True)

df["sentiment_int"] = df["Label"].map({"positivo": 1, "negativo": 0})

Lowercase e acenteuação

df['Texto'] = df.Texto.apply(str.lower)
df['Texto'] = df.Texto.apply(unidecode)

Retindo caracteres especiais

# Substitui nickname por espaço

df = df.replace(to_replace = '\@(\w{4,15})', value = ' ', regex = True)

# Substitui link por espaço

df = df.replace(to_replace ='http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', value = ' ', regex = True)

# Substitui por espaço todos caracteres que não forem de a a Z, dígitos de 0 a 9 e o caractere sublinhado _

df['Texto'] = df['Texto'].apply(lambda x : re.sub('\W', ' ', str(x)))
df['Label'] = df['Label'].apply(lambda x : re.sub('\W', ' ', str(x)))

# Substitui o caracter _ por espaço

df = df.replace(to_replace ='_', value = ' ', regex = True)

Stop word, lematização e tokenização

from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer

# Stop words

stop = stopwords.words('portuguese')

df['Texto'] = df['Texto'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

# Lematização

df['Texto'] = df['Texto'].apply(lambda row: " ".join([w.lemma_ for w in nlp(row)]))

# Teste

## Análise exploratória dos dados

df_test = pd.read_csv("../input/5cd-classificacao-emocao/6 - base_dados_teste_certificacao - Kaggle.csv", error_bad_lines=False)

df_test.head()

df_test.info()

df_test.isnull().sum()

## Pré - processamento

# Split da coluna "id;Texto;Label" separado por ;
new = df_test["id;texto"].str.split(";", n = 1, expand = True)

# Criando a Nova Coluna "Id" com o new[0]
df_test["Id"]= new[0] 

# Criando a Nova Coluna "Texto" com o new[1]
df_test["Texto"]= new[1]

# Retirando a antiga coluna "id;Texto;Label"
df_test.drop(columns =["id;texto"], inplace = True)

Lowercase e acenteuação

df_test['Texto'] = df_test.Texto.apply(str.lower)
df_test['Texto'] = df_test.Texto.apply(unidecode)

Retindo caracteres especiais

# Substitui nickname por espaço

df_test = df_test.replace(to_replace = '\@(\w{4,15})', value = ' ', regex = True)

# Substitui link por espaço

df_test = df_test.replace(to_replace ='http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', value = ' ', regex = True)

# Substitui por espaço todos caracteres que não forem de a a Z, dígitos de 0 a 9 e o caractere sublinhado _

df_test['Texto'] = df_test['Texto'].apply(lambda x : re.sub('\W', ' ', str(x)))

# Substitui o caracter _ por espaço

df_test = df_test.replace(to_replace ='_', value = ' ', regex = True)

Stop word, lematização e tokenização

from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer

# Stop words

stop = stopwords.words('portuguese')

df_test['Texto'] = df_test['Texto'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

# Lematização

df_test['Texto'] = df_test['Texto'].apply(lambda row: " ".join([w.lemma_ for w in nlp(row)]))

## Modelo

from sklearn import model_selection, preprocessing, linear_model, metrics
from sklearn.feature_extraction.text import CountVectorizer

Treino e validação - Hould out

train_x, test_x, train_y, test_y = model_selection.train_test_split(df["Texto"], df["sentiment_int"])

CountVectorizer para contar as palavras em cada tweet - frequencia positiva e negativa das palavras

vectorizer = CountVectorizer()
vectorizer.fit(train_x)

x_train_count = vectorizer.transform(train_x)
x_test_count = vectorizer.transform(test_x)

x_train_count.toarray()

Regressão logística

log = linear_model.LogisticRegression()
log_model = log.fit(x_train_count, train_y)

F1 score

scores = model_selection.cross_val_score(log_model, x_test_count, test_y, cv = 3, scoring="f1")
print(scores)

Teste

test_vect = vectorizer.transform(df_test.Texto)
test_vect = test_vect.toarray()

Predições

predictions = log_model.predict(test_vect)
predictions

predictions = pd.DataFrame(predictions)

predictions["sentimento"] = predictions[0].map({1 : "positivo", 0 : "negativo"})

predictions.insert(1, "id", range(1, 196), True)

predictions.drop([0], axis=1, inplace=True)
